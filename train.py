from sklearn.model_selection import KFold , train_test_split 
from transformers import AutoTokenizer , TrainerCallback
import pandas as pd
import numpy as np
from sklearn.metrics import f1_score, log_loss
from copy import deepcopy
from torch.nn import CrossEntropyLoss

#Creating Dataset of Torch object
import torch
class MalwareDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        input_ids = self.encodings['input_ids'][idx]
        attention_mask = self.encodings['attention_mask'][idx]
        pd.set_option('display.max_rows', None)
        
        label = self.labels[idx]
        input_ids = torch.tensor(input_ids)
        attention_mask = torch.tensor(attention_mask)
        label = torch.tensor(label)
        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label}

    def __len__(self):
        return len(self.labels)

#Creating TrainingArguments for model
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    logging_dir='./loglar',
    evaluation_strategy='epoch',
    save_strategy='epoch',
    logging_strategy="steps",
    num_train_epochs=150,              # total number of training epochs 
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    weight_decay=0.02,               # strength of weight decay
    logging_steps=20,
    overwrite_output_dir=True
)



#Defining how to calculate metrics for evaluate_dataset
def compute_metrics(logits_labels):
    logits, labels = logits_labels
    predictions = np.argmax(logits, axis=1)
    total_elements= labels.size
    matching_elements = np.sum(labels==predictions)
    acc = (matching_elements / total_elements)*100
    f1 = f1_score(labels, predictions, average= "micro")
    return { 'accuracy':acc, 'f1_score': f1 }


class CustomCallback(TrainerCallback):
    def __init__(self, trainer) -> None:
        super().__init__()
        self._trainer = trainer
    
    def on_epoch_end(self, args, state, control, **kwargs):
        if control.should_evaluate:
            control_copy = deepcopy(control)
            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset,metric_key_prefix="train")
            return control_copy 
        
#Reading and creating files
dfasm=pd.read_csv("output/trainAsmOutput.csv")
dfasm["ID"]= dfasm["ID"].astype("str")
Y=pd.read_csv("trainLabels.csv")

#Converting to string
result_asm = pd.merge(dfasm, Y,on="ID",how='left')
result_asm = result_asm.astype("str")

#Dropping unimportant and target features
asm_y = result_asm['Class']
asm_x = result_asm.drop(['ID','Class','.BSS:','rtn','.CODE','Unnamed: 52'], axis=1)


X_train_asm, X_test_asm, y_train_asm, y_test_asm = train_test_split(asm_x,asm_y ,test_size=0.20,shuffle=False)

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased",num_labels = 9)

#Function for tokenizing
def preprocess_function(examples):
    return tokenizer(examples,is_split_into_words=True,padding=True)

#Tokenizing the traing and test arrays
x_train_text_array = preprocess_function(X_train_asm.values.tolist())
x_test_text_array = preprocess_function(X_test_asm.values.tolist())

y_train_asm= y_train_asm.astype("int64")
y_test_asm= y_test_asm.astype("int64")

y_test_asm.reset_index(drop=True, inplace = True)

train_dataset = MalwareDataset(x_train_text_array, y_train_asm)
val_dataset = MalwareDataset(x_test_text_array, y_test_asm)

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,            # evaluation dataset
    compute_metrics=compute_metrics

)
trainer.add_callback(CustomCallback(trainer)) 

#Starting training
train_results = trainer.train()

#Saving trained model to ./model file
trainer.save_model(output_dir= "./model")

"""
#Shows only last epochs' metrics !!!
print("\n Train Results \n")
print(train_results)

metrics = train_results.metrics

print("\n Metrics \n")
print(metrics)
# save train results
trainer.log_metrics("eval", metrics)
trainer.save_metrics("eval", metrics)
"""

""" 
#Training with K-Fold Cross Validation
kf = KFold(n_splits=10, random_state=99, shuffle=True)
for train_index, val_index in kf.split(asm_x,asm_y):
    
    #Split
    train_base = asm_x.iloc[train_index]
    train_labels_base = asm_y.iloc[train_index]

    val_base = asm_x.iloc[val_index]
    val_labels_base = asm_y.iloc[val_index]

   #Function for tokenizing
    tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
    def preprocess_function(examples):
        return tokenizer(examples, truncation=True,is_split_into_words=True,padding=True, return_tensors="pt")

    train_encodings = preprocess_function(train_base.values.tolist())
    val_encodings = preprocess_function(val_base.values.tolist())

    #Converting target labels to int type
    y_train_asm= train_labels_base.astype("int64")
    y_val_asm= val_labels_base.astype("int64")
    
    y_train_asm.reset_index(drop=True, inplace = True)
    y_val_asm.reset_index(drop=True, inplace = True)


    # make datasets
    train_dataset = MalwareDataset(train_encodings, y_train_asm)
    val_dataset = MalwareDataset(val_encodings, y_val_asm)

    #Creating Model
    model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased",num_labels = 9)

    trainer = Trainer(
        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,         # training dataset
        eval_dataset=val_dataset,            # evaluation dataset
        compute_metrics=compute_metrics
    )

    trainer.train()

    #Saving trained model to ./model file
    trainer.save_model(output_dir= "./model") """