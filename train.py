from sklearn.model_selection import KFold
from transformers import AutoTokenizer
import pandas as pd
import numpy as np
from sklearn.metrics import f1_score

#Creating Dataset of Torch object
import torch
class MalwareDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        input_ids = self.encodings['input_ids'][idx]
        attention_mask = self.encodings['attention_mask'][idx]
        pd.set_option('display.max_rows', None)
        
        label = self.labels[idx]
        input_ids = torch.tensor(input_ids)
        attention_mask = torch.tensor(attention_mask)
        label = torch.tensor(label)
        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label}

    def __len__(self):
        return len(self.labels)

#Creating TrainingArguments for model
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    evaluation_strategy='epoch',
    save_strategy='epoch',
    num_train_epochs=10,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    weight_decay=0.02,               # strength of weight decay
)

#Defining how to calculate metrics for evaluate_dataset
def compute_metrics(logits_labels):
    logits, labels = logits_labels
    predictions = np.argmax(logits, axis=1)
    total_elements= labels.size
    matching_elements = np.sum(labels==predictions)
    acc = (matching_elements / total_elements)*100
    f1 = f1_score(labels, predictions, average= "micro")
    print(acc)
    print(f1)
    return {'accuracy':acc, 'f1_score': f1}

#Reading and creating files
dfasm=pd.read_csv("output/trainAsmOutput.csv")
dfasm["ID"]= dfasm["ID"].astype("str")
Y=pd.read_csv("trainLabels.csv")

#Converting to string
result_asm = pd.merge(dfasm, Y,on="ID",how='left')
result_asm = result_asm.astype("str")

#Dropping unimportant and target features
asm_y = result_asm['Class']
asm_x = result_asm.drop(['ID','Class','.BSS:','rtn','.CODE','Unnamed: 52'], axis=1)


kf = KFold(n_splits=10, random_state=99, shuffle=True)
for train_index, val_index in kf.split(asm_x,asm_y):
    
    #Split
    train_base = asm_x.iloc[train_index]
    train_labels_base = asm_y.iloc[train_index]

    val_base = asm_x.iloc[val_index]
    val_labels_base = asm_y.iloc[val_index]

   #Function for tokenizing
    tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
    def preprocess_function(examples):
        return tokenizer(examples, truncation=True,is_split_into_words=True,padding=True, return_tensors="pt")

    train_encodings = preprocess_function(train_base.values.tolist())
    val_encodings = preprocess_function(val_base.values.tolist())

    #Converting target labels to int type
    y_train_asm= train_labels_base.astype("int64")
    y_val_asm= val_labels_base.astype("int64")
    
    y_train_asm.reset_index(drop=True, inplace = True)
    y_val_asm.reset_index(drop=True, inplace = True)


    # make datasets
    train_dataset = MalwareDataset(train_encodings, y_train_asm)
    val_dataset = MalwareDataset(val_encodings, y_val_asm)

    #Creating Model
    model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased",num_labels = 9)

    trainer = Trainer(
        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,         # training dataset
        eval_dataset=val_dataset,            # evaluation dataset
        compute_metrics=compute_metrics
    )

    trainer.train()

    #Saving trained model to ./model file
    trainer.save_model(output_dir= "./model")