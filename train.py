import pandas as pd
from transformers import AutoTokenizer
from transformers import TrainingArguments, Trainer
from sklearn.model_selection import train_test_split
import numpy as np

#Reading and creating files
dfasm=pd.read_csv("output/firstAsmOutput.csv")
dfasm["ID"]= dfasm["ID"].astype("str")
Y=pd.read_csv("trainLabels.csv")

#Converting to string
result_asm = pd.merge(dfasm, Y,on="ID",how='left')
result_asm = result_asm.astype("str")

#Dropping unimportant and target features
asm_y = result_asm['Class']
asm_x = result_asm.drop(['ID','Class','.BSS:','rtn','.CODE','Unnamed: 52'], axis=1)

#Splitting the train file to test and train arrays
X_train_asm, X_test_asm, y_train_asm, y_test_asm = train_test_split(asm_x,asm_y ,test_size=0.20,shuffle=False)

#Choosing model and tokenizer
pretrained_model = "bert-base-uncased" 
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

#Function for tokenizing
def preprocess_function(examples):
    return tokenizer(examples, truncation=True,is_split_into_words=True,padding=True)

#Tokenizing the traing and test arrays
x_train_text_array = preprocess_function(X_train_asm.values.tolist())
x_test_text_array = preprocess_function(X_test_asm.values.tolist())

#Creating Dataset of Torch object
import torch
class MalwareDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        input_ids = self.encodings['input_ids'][idx]
        attention_mask = self.encodings['attention_mask'][idx]
        pd.set_option('display.max_rows', None)
        
        label = self.labels[idx]
        input_ids = torch.tensor(input_ids)
        attention_mask = torch.tensor(attention_mask)
        label = torch.tensor(label)
        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label}

    def __len__(self):
        return len(self.labels)

#Converting target labels to int type
y_train_asm= y_train_asm.astype("int64")
y_test_asm= y_test_asm.astype("int64")

#Reseting index for matching Dataset indexes and target label indexes
y_test_asm.reset_index(drop=True, inplace = True)

#Creating Datasets
train_dataset = MalwareDataset(x_train_text_array, y_train_asm)
val_dataset = MalwareDataset(x_test_text_array, y_test_asm)


#Creating TrainingArguments for model
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    evaluation_strategy='epoch',
    save_strategy='epoch',
    num_train_epochs=4,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    weight_decay=0.01,               # strength of weight decay
)

#Creating Model
model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased",num_labels = 9)

from sklearn.metrics import f1_score, accuracy_score, confusion_matrix

""" 
#Defining how to calculate metrics for evaluate_dataset
def compute_metrics(logits_labels):
    logits, labels = logits_labels
    predictions = np.argmax(logits, axis=1)
    total_elements= labels.size
    matching_elements = np.sum(labels==predictions)
    acc = (matching_elements / total_elements)*100
    #acc = np.mean(labels)
    f1 = f1_score(labels, predictions, average= "micro")
    print(logits)
    print(predictions)
    print(labels)
    print(acc)
    print(f1)
    return {'accuracy':acc, 'f1_score': f1}

#Creating Trainer object with pre-defined model, training-args, compute_metrics and datasets
trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,            # evaluation dataset
    compute_metrics=compute_metrics

)

#Starting training
trainer.train()

#Saving trained model to ./model file
trainer.save_model(output_dir= "./model")
"""

#Importing the model that we trained and creaing model object
model_path = "./model"
model = AutoModelForSequenceClassification.from_pretrained(model_path)

#Tokenizing the test array
x_test_text_array = tokenizer(X_test_asm.values.tolist(), return_tensors="pt",truncation=True,is_split_into_words=True,padding=True)

#Testing the model with test_dataset and calculating the accuracy
with torch.no_grad():
    outputs = model(**x_test_text_array)
    predictions = np.argmax(outputs.logits, axis=1)
    matching_elements = np.sum(y_test_asm.to_numpy()==predictions.numpy())
    total_elements = y_test_asm.size
    acc = (matching_elements / total_elements)*100
    print(predictions)
    print(y_test_asm.to_numpy())
    print(acc)


