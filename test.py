from transformers import AutoModelForSequenceClassification , AutoTokenizer  
import torch
import numpy as np
import pandas as pd

#Reading and creating files
dfasm=pd.read_csv("output/testAsmOutput.csv")
dfasm["ID"]= dfasm["ID"].astype("str")
Y=pd.read_csv("trainLabels.csv")

result_asm = pd.merge(dfasm, Y,on="ID",how='left')
result_asm = result_asm.astype("str")

asm_y = result_asm['Class'].astype("int64")
asm_x = result_asm.drop(['ID','Class','.BSS:','rtn','.CODE','Unnamed: 52'], axis=1)


#Importing the model that we trained and creaing model object
model_path = "./model"
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

#Tokenizing the test array
x_test_text_array = tokenizer(asm_x.values.tolist(), return_tensors="pt",is_split_into_words=True,padding=True)


#Testing the model with test_dataset and calculating the accuracy
with torch.no_grad():
    outputs = model(**x_test_text_array)
    predictions = np.argmax(outputs.logits, axis=1)
    matching_elements = np.sum(asm_y.to_numpy()==predictions.numpy())
    total_elements = asm_y.size
    acc = (matching_elements / total_elements)*100
    print(predictions.numpy())
    print(asm_y.to_numpy())
    print(acc)
 
